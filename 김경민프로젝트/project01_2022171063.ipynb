{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "project 1",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:56:14.073796Z",
     "start_time": "2025-11-28T14:56:14.067147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             precision_score, recall_score, f1_score,\n",
    "                             precision_recall_curve, roc_auc_score, roc_curve, make_scorer)\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:56:14.097626Z",
     "start_time": "2025-11-28T14:56:14.086826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#==============================================================================\n",
    "# PROBLEM 1: Calculate profits under different scenarios\n",
    "#==============================================================================\n",
    "\n",
    "def calculate_profit(y_true, y_pred, P=100, Q=1000, S=2000):\n",
    "\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))  # Correctly identified defects\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))  # Correctly identified normals\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))  # Normal wrongly marked as defect\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))  # Defect wrongly marked as normal\n",
    "\n",
    "    # Profit calculation:\n",
    "    # TP: Defects discarded without test -> lose P (100)\n",
    "    # TN: Normals tested and sold -> profit (S - P - Q) = 900\n",
    "    # FP: Normals discarded -> lose (S - P - Q + P) = lose potential profit + P\n",
    "    # FN: Defects tested -> lose (P + Q) = 1100\n",
    "\n",
    "    profit_TP = TP * (-P)  # Discarded defects: -100 each\n",
    "    profit_TN = TN * (S - P - Q)  # Sold normals: +900 each\n",
    "    profit_FP = FP * (-P)  # Wrongly discarded normals: -100 each (but lost 900 profit)\n",
    "    profit_FN = FN * (-(P + Q))  # Tested defects: -1100 each\n",
    "\n",
    "    total_profit = profit_TP + profit_TN + profit_FP + profit_FN\n",
    "\n",
    "    return total_profit\n",
    "\n",
    "def problem1_analysis(y_train, P=100, Q=1000, S=2000):\n",
    "    \"\"\"\n",
    "    Problem 1: Calculate profits for different scenarios\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"PROBLEM 1: Profit Analysis\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    n_total = len(y_train)\n",
    "    n_normal = np.sum(y_train == 0)\n",
    "    n_defect = np.sum(y_train == 1)\n",
    "    defect_rate = n_defect / n_total\n",
    "\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"Total samples: {n_total}\")\n",
    "    print(f\"Normal: {n_normal} ({n_normal/n_total*100:.2f}%)\")\n",
    "    print(f\"Defect: {n_defect} ({n_defect/n_total*100:.2f}%)\")\n",
    "\n",
    "    # Scenario 1: Test all chips (no model)\n",
    "    profit_no_model = n_normal * (S - P - Q) + n_defect * (-(P + Q))\n",
    "    print(f\"\\n1) No model (test all): {profit_no_model:,} won\")\n",
    "\n",
    "    # Scenario 2: Perfect model\n",
    "    profit_perfect = n_normal * (S - P - Q) + n_defect * (-P)\n",
    "    print(f\"2) Perfect model: {profit_perfect:,} won\")\n",
    "    print(f\"   Improvement: {profit_perfect - profit_no_model:,} won\")\n",
    "\n",
    "    # Scenario 3: 90% Precision and 90% Recall model\n",
    "    recall_90 = 0.90\n",
    "    precision_90 = 0.90\n",
    "\n",
    "    # With 90% recall: we correctly identify 90% of defects\n",
    "    TP_90 = int(n_defect * recall_90)\n",
    "    FN_90 = n_defect - TP_90\n",
    "\n",
    "    # With 90% precision: among predicted defects, 90% are actually defects\n",
    "    # Precision = TP / (TP + FP) = 0.9\n",
    "    # So: TP = 0.9 * (TP + FP)\n",
    "    # FP = TP / 0.9 - TP = TP * (1/0.9 - 1)\n",
    "    FP_90 = int(TP_90 * (1/precision_90 - 1))\n",
    "    TN_90 = n_normal - FP_90\n",
    "\n",
    "    profit_90 = (TP_90 * (-P) + TN_90 * (S - P - Q) +\n",
    "                 FP_90 * (-P) + FN_90 * (-(P + Q)))\n",
    "\n",
    "    print(f\"3) 90% Recall & 90% Precision model: {profit_90:,} won\")\n",
    "    print(f\"   TP: {TP_90}, TN: {TN_90}, FP: {FP_90}, FN: {FN_90}\")\n",
    "    print(f\"   Improvement over no model: {profit_90 - profit_no_model:,} won\")\n",
    "\n",
    "    return profit_no_model, profit_perfect, profit_90"
   ],
   "id": "c7c7726b3952a1cc",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:56:14.133889Z",
     "start_time": "2025-11-28T14:56:14.111637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#==============================================================================\n",
    "# PROBLEM 2: EDA and Preprocessing\n",
    "#==============================================================================\n",
    "\n",
    "def load_and_explore_data(train_path):\n",
    "    \"\"\"\n",
    "    Load data and perform initial exploration\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROBLEM 2: EDA and Preprocessing\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Load data\n",
    "    df_train = pd.read_csv(train_path)\n",
    "\n",
    "    print(f\"\\nOriginal data shape: {df_train.shape}\")\n",
    "    print(f\"Columns: Label + {df_train.shape[1]-1} sensor variables\")\n",
    "\n",
    "    # Separate features and labels\n",
    "    y = df_train['Label'].map({'normal': 0, 'defect': 1})\n",
    "    X = df_train.drop('Label', axis=1)\n",
    "\n",
    "    return X, y, df_train\n",
    "\n",
    "def comprehensive_eda(X, y):\n",
    "    \"\"\"\n",
    "    Perform comprehensive EDA\n",
    "    \"\"\"\n",
    "    print(\"\\n--- EDA: Missing Values ---\")\n",
    "    missing_counts = X.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(X)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_counts,\n",
    "        'Missing_Percentage': missing_pct\n",
    "    }).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "    print(f\"Variables with missing values: {(missing_counts > 0).sum()}\")\n",
    "    print(\"\\nTop 10 variables with most missing values:\")\n",
    "    print(missing_df.head(10))\n",
    "\n",
    "    X_temp = X.copy()\n",
    "    # Temporarily fill NAs to check variance\n",
    "    X_temp = X_temp.fillna(0)\n",
    "\n",
    "    # Check for zero-variance features\n",
    "    print(\"\\n--- EDA: Zero-Variance Features ---\")\n",
    "    zero_var_cols = []\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype in ['int64', 'float64']:\n",
    "            if X[col].std() == 0 or X[col].nunique() == 1:\n",
    "                zero_var_cols.append(col)\n",
    "\n",
    "    print(f\"Zero-variance features: {len(zero_var_cols)}\")\n",
    "    if len(zero_var_cols) > 0:\n",
    "        print(f\"Examples: {zero_var_cols[:5]}\")\n",
    "\n",
    "    # Check data types\n",
    "    print(\"\\n--- EDA: Data Types ---\")\n",
    "    print(X.dtypes.value_counts())\n",
    "\n",
    "    # Class distribution\n",
    "    print(\"\\n--- EDA: Class Distribution ---\")\n",
    "    print(y.value_counts())\n",
    "    print(f\"Imbalance ratio: {y.value_counts()[0] / y.value_counts()[1]:.2f}:1\")\n",
    "\n",
    "    return missing_df, zero_var_cols\n",
    "\n",
    "\n",
    "def preprocess_data(X, y, missing_df=None, zero_var_cols=None,\n",
    "                   train_imputer=None, train_scaler=None,\n",
    "                   zero_var_fill_map=None, is_train=True):\n",
    "    \"\"\"\n",
    "    Preprocess data with proper handling for train/test\n",
    "\n",
    "    Args:\n",
    "        X: features\n",
    "        y: labels (None for test data)\n",
    "        missing_df: missing value statistics from training\n",
    "        zero_var_cols: zero variance columns from training\n",
    "        train_imputer: fitted imputer from training\n",
    "        train_scaler: fitted scaler from training\n",
    "        zero_var_fill_map: mapping of zero-var columns to fill values (0 or 1)\n",
    "        is_train: whether this is training data\n",
    "    \"\"\"\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    # Step 1: Handle zero-variance columns - fill NAs based on dominant value\n",
    "    if is_train and zero_var_cols is not None and len(zero_var_cols) > 0:\n",
    "        zero_var_fill_map = {}\n",
    "        print(f\"\\n--- Handling Zero-Variance Columns ---\")\n",
    "        print(f\"Found {len(zero_var_cols)} zero-variance columns\")\n",
    "\n",
    "        for col in zero_var_cols:\n",
    "            if col in X_processed.columns:\n",
    "                # Get non-null values\n",
    "                non_null_values = X_processed[col].dropna()\n",
    "\n",
    "                if len(non_null_values) > 0:\n",
    "                    # Count zeros and non-zeros\n",
    "                    zero_count = (non_null_values == 0).sum()\n",
    "                    non_zero_count = (non_null_values != 0).sum()\n",
    "\n",
    "                    # If dominantly zero, fill NAs with 1, otherwise fill with 0\n",
    "                    if zero_count > non_zero_count:\n",
    "                        fill_value = 1\n",
    "                    else:\n",
    "                        fill_value = 0\n",
    "\n",
    "                    zero_var_fill_map[col] = fill_value\n",
    "                    X_processed[col].fillna(fill_value, inplace=True)\n",
    "                else:\n",
    "                    # If all values are null, fill with 0\n",
    "                    zero_var_fill_map[col] = 0\n",
    "                    X_processed[col].fillna(0, inplace=True)\n",
    "\n",
    "        # Now remove zero-variance columns\n",
    "        cols_to_remove = [col for col in zero_var_cols if col in X_processed.columns]\n",
    "        X_processed = X_processed.drop(columns=cols_to_remove, errors='ignore')\n",
    "        print(f\"Removed {len(cols_to_remove)} zero-variance features after filling NAs\")\n",
    "\n",
    "        # Store the final column list after removal\n",
    "        final_columns = X_processed.columns.tolist()\n",
    "\n",
    "    elif not is_train and zero_var_fill_map is not None:\n",
    "        # For test data: apply same fill strategy, then remove columns\n",
    "        for col, fill_value in zero_var_fill_map.items():\n",
    "            if col in X_processed.columns:\n",
    "                X_processed[col].fillna(fill_value, inplace=True)\n",
    "\n",
    "        # Remove the same zero-variance columns\n",
    "        if zero_var_cols is not None:\n",
    "            cols_to_remove = [col for col in zero_var_cols if col in X_processed.columns]\n",
    "            X_processed = X_processed.drop(columns=cols_to_remove, errors='ignore')\n",
    "    else:\n",
    "        final_columns = None\n",
    "\n",
    "    # Step 2: Handle missing values for remaining columns\n",
    "    if is_train:\n",
    "        # For training: fit imputer based on missing percentage\n",
    "        high_missing_cols = missing_df[missing_df['Missing_Percentage'] > 50].index.tolist()\n",
    "        # Only consider columns that still exist after zero-var removal\n",
    "        high_missing_cols = [col for col in high_missing_cols if col in X_processed.columns]\n",
    "\n",
    "        low_missing_cols = [col for col in X_processed.columns\n",
    "                           if col not in high_missing_cols and X_processed[col].isnull().any()]\n",
    "\n",
    "        print(f\"\\n--- Missing Value Handling (Non-Zero-Variance Columns) ---\")\n",
    "        print(f\"  - High missing (>50%, fill with 0): {len(high_missing_cols)} columns\")\n",
    "        print(f\"  - Low missing (≤50%, fill with mean): {len(low_missing_cols)} columns\")\n",
    "\n",
    "        # Fill high missing with 0\n",
    "        for col in high_missing_cols:\n",
    "            X_processed[col].fillna(0, inplace=True)\n",
    "\n",
    "        # Fill low missing with mean\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        if low_missing_cols:\n",
    "            X_processed[low_missing_cols] = imputer.fit_transform(X_processed[low_missing_cols])\n",
    "\n",
    "        train_imputer = {'high_missing': high_missing_cols,\n",
    "                        'low_missing': low_missing_cols,\n",
    "                        'imputer': imputer,\n",
    "                        'final_columns': final_columns}\n",
    "    else:\n",
    "        # For test: use training imputer\n",
    "        high_missing_cols = train_imputer['high_missing']\n",
    "        low_missing_cols = train_imputer['low_missing']\n",
    "        imputer = train_imputer['imputer']\n",
    "        final_columns = train_imputer['final_columns']\n",
    "\n",
    "        for col in high_missing_cols:\n",
    "            if col in X_processed.columns:\n",
    "                X_processed[col].fillna(0, inplace=True)\n",
    "\n",
    "        if low_missing_cols:\n",
    "            cols_to_impute = [col for col in low_missing_cols if col in X_processed.columns]\n",
    "            if cols_to_impute:\n",
    "                X_processed[cols_to_impute] = imputer.transform(X_processed[cols_to_impute])\n",
    "\n",
    "        # Ensure test data has same columns as training data\n",
    "        # Add missing columns with 0\n",
    "        for col in final_columns:\n",
    "            if col not in X_processed.columns:\n",
    "                X_processed[col] = 0\n",
    "\n",
    "        # Keep only columns that were in training\n",
    "        X_processed = X_processed[final_columns]\n",
    "\n",
    "    # Step 3: Standardization\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_processed)\n",
    "        train_scaler = scaler\n",
    "    else:\n",
    "        X_scaled = train_scaler.transform(X_processed)\n",
    "\n",
    "    X_final = pd.DataFrame(X_scaled, columns=X_processed.columns, index=X_processed.index)\n",
    "\n",
    "    if is_train:\n",
    "        print(f\"\\nFinal preprocessed data shape: {X_final.shape}\")\n",
    "        print(f\"Samples: {X_final.shape[0]}, Features: {X_final.shape[1]}\")\n",
    "        return X_final, y, train_imputer, train_scaler, zero_var_fill_map\n",
    "    else:\n",
    "        return X_final\n",
    "\n",
    "    # Step 3: Standardization\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_processed)\n",
    "        train_scaler = scaler\n",
    "    else:\n",
    "        X_scaled = train_scaler.transform(X_processed)\n",
    "\n",
    "    X_final = pd.DataFrame(X_scaled, columns=X_processed.columns, index=X_processed.index)\n",
    "\n",
    "    if is_train:\n",
    "        print(f\"\\nFinal preprocessed data shape: {X_final.shape}\")\n",
    "        print(f\"Samples: {X_final.shape[0]}, Features: {X_final.shape[1]}\")\n",
    "        return X_final, y, train_imputer, train_scaler, zero_var_fill_map\n",
    "    else:\n",
    "        return X_final"
   ],
   "id": "c09f28ae26930937",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:56:14.169051Z",
     "start_time": "2025-11-28T14:56:14.146072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#==============================================================================\n",
    "# PROBLEM 3: Model Development and Selection\n",
    "#==============================================================================\n",
    "\n",
    "def find_optimal_threshold(y_true, y_proba, target_precision=0.90, P=100, Q=1000, S=2000):\n",
    "    \"\"\"\n",
    "    Find optimal threshold balancing precision target and profit\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.01, 0.99, 0.01)\n",
    "    results = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        if np.sum(y_pred) == 0:  # No defects predicted\n",
    "            continue\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        profit = calculate_profit(y_true, y_pred, P, Q, S)\n",
    "\n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'profit': profit\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Find threshold with precision >= target_precision and maximum profit\n",
    "    high_precision = results_df[results_df['precision'] >= target_precision]\n",
    "\n",
    "    if len(high_precision) > 0:\n",
    "        best_idx = high_precision['profit'].idxmax()\n",
    "        best_threshold = results_df.loc[best_idx, 'threshold']\n",
    "        best_precision = results_df.loc[best_idx, 'precision']\n",
    "        best_recall = results_df.loc[best_idx, 'recall']\n",
    "        best_profit = results_df.loc[best_idx, 'profit']\n",
    "    else:\n",
    "        # If no threshold achieves target precision, pick highest precision\n",
    "        best_idx = results_df['precision'].idxmax()\n",
    "        best_threshold = results_df.loc[best_idx, 'threshold']\n",
    "        best_precision = results_df.loc[best_idx, 'precision']\n",
    "        best_recall = results_df.loc[best_idx, 'recall']\n",
    "        best_profit = results_df.loc[best_idx, 'profit']\n",
    "\n",
    "    return best_threshold, best_precision, best_recall, best_profit, results_df\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, target_precision=0.90):\n",
    "    \"\"\"\n",
    "    Train multiple models and select the best one based on F1 score\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROBLEM 3: Model Development\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Split for validation\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTrain set: {X_tr.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "\n",
    "    # Define individual models\n",
    "    models = {\n",
    "        'Logistic Regression (L2)': LogisticRegression(\n",
    "            penalty='l2', C=1.0, max_iter=1000, random_state=42, class_weight='balanced'\n",
    "        ),\n",
    "        'KNN (k=5)': KNeighborsClassifier(\n",
    "            n_neighbors=5, weights='distance', metric='euclidean'\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, random_state=42, class_weight='balanced'\n",
    "        ),\n",
    "\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "\n",
    "    # Train and evaluate individual models\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training: {name} ---\")\n",
    "\n",
    "        # Train\n",
    "        model.fit(X_tr, y_tr)\n",
    "        trained_models[name] = model\n",
    "\n",
    "        # Predict probabilities\n",
    "        y_tr_proba = model.predict_proba(X_tr)[:, 1]\n",
    "        y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Find optimal threshold based on F1 score\n",
    "        thresholds = np.arange(0.01, 0.99, 0.1)\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        best_metrics = {}\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            y_val_pred = (y_val_proba >= threshold).astype(int)\n",
    "\n",
    "            # Skip if no predictions of either class\n",
    "            if len(np.unique(y_val_pred)) < 2:\n",
    "                continue\n",
    "\n",
    "            f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "                best_metrics = {\n",
    "                    'precision': precision_score(y_val, y_val_pred),\n",
    "                    'recall': recall_score(y_val, y_val_pred),\n",
    "                    'f1': f1,\n",
    "                    'profit': calculate_profit(y_val, y_val_pred)\n",
    "                }\n",
    "\n",
    "        # Calculate AUC *Area Under Curve\n",
    "        val_auc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "        print(f\"Optimal threshold: {best_threshold:.3f}\")\n",
    "        print(f\"Validation - Precision: {best_metrics['precision']:.4f}, Recall: {best_metrics['recall']:.4f}\")\n",
    "        print(f\"Validation - F1: {best_metrics['f1']:.4f}, AUC: {val_auc:.4f}\")\n",
    "        print(f\"Validation - Profit: {best_metrics['profit']:,} won\")\n",
    "\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'threshold': best_threshold,\n",
    "            'val_precision': best_metrics['precision'],\n",
    "            'val_recall': best_metrics['recall'],\n",
    "            'val_f1': best_metrics['f1'],\n",
    "            'val_auc': val_auc,\n",
    "            'val_profit': best_metrics['profit']\n",
    "        }\n",
    "\n",
    "    # Build Ensemble Model from top 3 models based on F1 score\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Building Ensemble Model (Voting Classifier)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Sort models by F1 score\n",
    "    sorted_models = sorted(results.items(), key=lambda x: x[1]['val_f1'], reverse=True)\n",
    "    top_3_names = [name for name, _ in sorted_models[:3]]\n",
    "\n",
    "    print(f\"\\nTop 3 models by F1 score:\")\n",
    "    for i, name in enumerate(top_3_names, 1):\n",
    "        print(f\"{i}. {name} (F1: {results[name]['val_f1']:.4f})\")\n",
    "\n",
    "    # Create ensemble\n",
    "    ensemble_estimators = [(name, trained_models[name]) for name in top_3_names]\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=ensemble_estimators,\n",
    "        voting='soft',  # Use probability averaging\n",
    "        weights=None    # Equal weights\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Training: Ensemble Model ---\")\n",
    "    ensemble.fit(X_tr, y_tr)\n",
    "\n",
    "    # Evaluate ensemble\n",
    "    y_val_proba_ensemble = ensemble.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Find optimal threshold for ensemble based on F1\n",
    "    best_f1_ensemble = 0\n",
    "    best_threshold_ensemble = 0.5\n",
    "    best_metrics_ensemble = {}\n",
    "\n",
    "    for threshold in np.arange(0.01, 0.99, 0.01):\n",
    "        y_val_pred = (y_val_proba_ensemble >= threshold).astype(int)\n",
    "\n",
    "        if len(np.unique(y_val_pred)) < 2:\n",
    "            continue\n",
    "\n",
    "        f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "        if f1 > best_f1_ensemble:\n",
    "            best_f1_ensemble = f1\n",
    "            best_threshold_ensemble = threshold\n",
    "            best_metrics_ensemble = {\n",
    "                'precision': precision_score(y_val, y_val_pred),\n",
    "                'recall': recall_score(y_val, y_val_pred),\n",
    "                'f1': f1,\n",
    "                'profit': calculate_profit(y_val, y_val_pred)\n",
    "            }\n",
    "\n",
    "    val_auc_ensemble = roc_auc_score(y_val, y_val_proba_ensemble)\n",
    "\n",
    "    print(f\"Optimal threshold: {best_threshold_ensemble:.3f}\")\n",
    "    print(f\"Validation - Precision: {best_metrics_ensemble['precision']:.4f}, Recall: {best_metrics_ensemble['recall']:.4f}\")\n",
    "    print(f\"Validation - F1: {best_metrics_ensemble['f1']:.4f}, AUC: {val_auc_ensemble:.4f}\")\n",
    "    print(f\"Validation - Profit: {best_metrics_ensemble['profit']:,} won\")\n",
    "\n",
    "    # Add ensemble to results\n",
    "    results['Ensemble (Top 3)'] = {\n",
    "        'model': ensemble,\n",
    "        'threshold': best_threshold_ensemble,\n",
    "        'val_precision': best_metrics_ensemble['precision'],\n",
    "        'val_recall': best_metrics_ensemble['recall'],\n",
    "        'val_f1': best_metrics_ensemble['f1'],\n",
    "        'val_auc': val_auc_ensemble,\n",
    "        'val_profit': best_metrics_ensemble['profit']\n",
    "    }\n",
    "\n",
    "    # Select best model based on F1 score\n",
    "    best_model_name = max(results, key=lambda x: results[x]['val_f1'])\n",
    "    best_model_info = results[best_model_name]\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BEST MODEL (by F1 score): {best_model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Threshold: {best_model_info['threshold']:.3f}\")\n",
    "    print(f\"Validation Precision: {best_model_info['val_precision']:.4f}\")\n",
    "    print(f\"Validation Recall: {best_model_info['val_recall']:.4f}\")\n",
    "    print(f\"Validation F1: {best_model_info['val_f1']:.4f}\")\n",
    "    print(f\"Validation AUC: {best_model_info['val_auc']:.4f}\")\n",
    "    print(f\"Validation Profit: {best_model_info['val_profit']:,} won\")\n",
    "\n",
    "    return best_model_info['model'], best_model_info['threshold'], results, X_val, y_val"
   ],
   "id": "8c4dd57ad13c35dd",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:56:14.188123Z",
     "start_time": "2025-11-28T14:56:14.179656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#==============================================================================\n",
    "# PROBLEM 4: Predict on test data\n",
    "#==============================================================================\n",
    "\n",
    "def predict_test_data(model, threshold, X_test, output_file):\n",
    "    \"\"\"\n",
    "    Predict on test data and save results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROBLEM 4: Test Data Prediction\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred = (y_test_proba >= threshold).astype(int)\n",
    "\n",
    "    # Save predictions\n",
    "    with open(output_file, 'w') as f:\n",
    "        for pred in y_test_pred:\n",
    "            f.write(f\"{pred}\\n\")\n",
    "\n",
    "    print(f\"\\nPredictions saved to: {output_file}\")\n",
    "    print(f\"Total predictions: {len(y_test_pred)}\")\n",
    "    print(f\"Predicted normal: {np.sum(y_test_pred == 0)}\")\n",
    "    print(f\"Predicted defect: {np.sum(y_test_pred == 1)}\")\n",
    "\n",
    "    return y_test_pred"
   ],
   "id": "ed32ebf57e6ceb92",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T14:56:14.210200Z",
     "start_time": "2025-11-28T14:56:14.198091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#==============================================================================\n",
    "# PROBLEM 5: Evaluate model on full training data\n",
    "#==============================================================================\n",
    "\n",
    "def evaluate_on_full_training(model, threshold, X_train, y_train,\n",
    "                              profit_no_model, profit_perfect):\n",
    "    \"\"\"\n",
    "    Evaluate model on full training data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROBLEM 5: Model Evaluation on Training Data\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "\n",
    "    precision = precision_score(y_train, y_train_pred)\n",
    "    recall = recall_score(y_train, y_train_pred)\n",
    "    f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "    profit_model = calculate_profit(y_train, y_train_pred)\n",
    "    improvement = profit_model - profit_no_model\n",
    "    pct_of_perfect = (profit_model - profit_no_model) / (profit_perfect - profit_no_model) * 100\n",
    "\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    print(f\"\\nProfit Comparison:\")\n",
    "    print(f\"No model profit: {profit_no_model:,} won\")\n",
    "    print(f\"Model profit: {profit_model:,} won\")\n",
    "    print(f\"Perfect model profit: {profit_perfect:,} won\")\n",
    "    print(f\"\\nImprovement over no model: {improvement:,} won\")\n",
    "    print(f\"Percentage of perfect model gain: {pct_of_perfect:.2f}%\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_train, y_train_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                Predicted Normal  Predicted Defect\")\n",
    "    print(f\"Actual Normal          {cm[0,0]:6d}          {cm[0,1]:6d}\")\n",
    "    print(f\"Actual Defect          {cm[1,0]:6d}          {cm[1,1]:6d}\")\n",
    "\n",
    "    return profit_model\n",
    "\n",
    "#==============================================================================\n",
    "# PROBLEM 6 & 7: Price change scenarios\n",
    "#==============================================================================\n",
    "\n",
    "def optimize_for_new_price(model, X_train, y_train, S_new, P=100, Q=1000):\n",
    "    \"\"\"\n",
    "    Optimize model for new selling price\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- New Price Scenario: S = {S_new} won ---\")\n",
    "\n",
    "    # Calculate baseline profits\n",
    "    y_all_test = np.zeros(len(y_train))  # Predict all as normal (test all)\n",
    "    profit_no_model = calculate_profit(y_train, y_all_test, P, Q, S_new)\n",
    "\n",
    "    y_perfect = y_train.copy()\n",
    "    profit_perfect = calculate_profit(y_train, y_perfect, P, Q, S_new)\n",
    "\n",
    "    print(f\"\\nNo model profit: {profit_no_model:,} won\")\n",
    "    print(f\"Perfect model profit: {profit_perfect:,} won\")\n",
    "\n",
    "    # Find optimal threshold for new price\n",
    "    y_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "    thresholds = np.arange(0.01, 0.99, 0.01)\n",
    "    best_profit = float('-inf')\n",
    "    best_threshold = 0.5\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        profit = calculate_profit(y_train, y_pred, P, Q, S_new)\n",
    "\n",
    "        if profit > best_profit:\n",
    "            best_profit = profit\n",
    "            best_threshold = threshold\n",
    "            if np.sum(y_pred) > 0:\n",
    "                best_precision = precision_score(y_train, y_pred)\n",
    "                best_recall = recall_score(y_train, y_pred)\n",
    "\n",
    "    print(f\"\\nOptimized Model:\")\n",
    "    print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Precision: {best_precision:.4f}\")\n",
    "    print(f\"Recall: {best_recall:.4f}\")\n",
    "    print(f\"Model profit: {best_profit:,} won\")\n",
    "    print(f\"Improvement over no model: {best_profit - profit_no_model:,} won\")\n",
    "    print(f\"Percentage of perfect gain: {(best_profit - profit_no_model)/(profit_perfect - profit_no_model)*100:.2f}%\")\n",
    "\n",
    "    return best_threshold, best_profit\n"
   ],
   "id": "e78b4019670392bc",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-28T14:56:14.225607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    # File paths - UPDATE THESE\n",
    "    train_path = 'data98_semi_train.csv'\n",
    "    test_path = 'data98_semi_test.csv'\n",
    "    output_file = 'project01_2022171063.txt'\n",
    "\n",
    "    # Load and explore data\n",
    "    X, y, df_train = load_and_explore_data(train_path)\n",
    "\n",
    "    # EDA\n",
    "    missing_df, zero_var_cols = comprehensive_eda(X, y)\n",
    "\n",
    "    # Problem 1\n",
    "    profit_no_model, profit_perfect, profit_90 = problem1_analysis(y)\n",
    "\n",
    "    # Preprocess training data\n",
    "    X_processed, y_processed, train_imputer, train_scaler, zero_var_fill_map = preprocess_data(\n",
    "        X, y, missing_df, zero_var_cols, is_train=True\n",
    "    )\n",
    "\n",
    "    # Problem 3: Train and select model\n",
    "    best_model, best_threshold, all_results, X_val, y_val = train_and_evaluate_models(\n",
    "        X_processed, y_processed, target_precision=0.95\n",
    "    )\n",
    "\n",
    "    # Problem 5: Evaluate on training data\n",
    "    profit_model = evaluate_on_full_training(\n",
    "        best_model, best_threshold, X_processed, y_processed,\n",
    "        profit_no_model, profit_perfect\n",
    "    )\n",
    "\n",
    "    # Load and preprocess test data\n",
    "    print(\"\\n--- Loading test data ---\")\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    X_test = df_test.copy()\n",
    "\n",
    "    # Preprocess test data using training statistics\n",
    "    X_test_processed = preprocess_data(\n",
    "        X_test, None, missing_df, zero_var_cols,\n",
    "        train_imputer, train_scaler, zero_var_fill_map, is_train=False\n",
    "    )\n",
    "\n",
    "    # Problem 4: Predict on test data\n",
    "    y_test_pred = predict_test_data(best_model, best_threshold, X_test_processed, output_file)\n",
    "\n",
    "    # Problem 6: Price drops to 1,200 won\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROBLEM 6: Price Change to S = 1,200 won\")\n",
    "    print(\"=\"*80)\n",
    "    threshold_1200, profit_1200 = optimize_for_new_price(\n",
    "        best_model, X_processed, y_processed, S_new=1200\n",
    "    )\n",
    "\n",
    "    # Problem 7: Price increases to 5,000 won\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROBLEM 7: Price Change to S = 5,000 won\")\n",
    "    print(\"=\"*80)\n",
    "    threshold_5000, profit_5000 = optimize_for_new_price(\n",
    "        best_model, X_processed, y_processed, S_new=5000\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"1. Original model (S=2000): Threshold={best_threshold:.3f}, Profit={profit_model:,}\")\n",
    "    print(f\"2. Optimized for S=1200: Threshold={threshold_1200:.3f}, Profit={profit_1200:,}\")\n",
    "    print(f\"3. Optimized for S=5000: Threshold={threshold_5000:.3f}, Profit={profit_5000:,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "4ea48be0f7fa9c5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROBLEM 2: EDA and Preprocessing\n",
      "================================================================================\n",
      "\n",
      "Original data shape: (11491, 591)\n",
      "Columns: Label + 590 sensor variables\n",
      "\n",
      "--- EDA: Missing Values ---\n",
      "Variables with missing values: 538\n",
      "\n",
      "Top 10 variables with most missing values:\n",
      "      Missing_Count  Missing_Percentage\n",
      "v158          10489           91.280132\n",
      "v294          10465           91.071273\n",
      "v293          10455           90.984249\n",
      "v159          10448           90.923331\n",
      "v086           9839           85.623531\n",
      "v221           9818           85.440780\n",
      "v493           9808           85.353755\n",
      "v359           9796           85.249326\n",
      "v111           7477           65.068314\n",
      "v110           7469           64.998695\n",
      "\n",
      "--- EDA: Zero-Variance Features ---\n",
      "Zero-variance features: 116\n",
      "Examples: ['v006', 'v014', 'v043', 'v050', 'v053']\n",
      "\n",
      "--- EDA: Data Types ---\n",
      "float64    590\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- EDA: Class Distribution ---\n",
      "Label\n",
      "0    10751\n",
      "1      740\n",
      "Name: count, dtype: int64\n",
      "Imbalance ratio: 14.53:1\n",
      "================================================================================\n",
      "PROBLEM 1: Profit Analysis\n",
      "================================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples: 11491\n",
      "Normal: 10751 (93.56%)\n",
      "Defect: 740 (6.44%)\n",
      "\n",
      "1) No model (test all): 8,861,900 won\n",
      "2) Perfect model: 9,601,900 won\n",
      "   Improvement: 740,000 won\n",
      "3) 90% Recall & 90% Precision model: 9,453,900 won\n",
      "   TP: 666, TN: 10677, FP: 74, FN: 74\n",
      "   Improvement over no model: 592,000 won\n",
      "\n",
      "--- Handling Zero-Variance Columns ---\n",
      "Found 116 zero-variance columns\n",
      "Removed 116 zero-variance features after filling NAs\n",
      "\n",
      "--- Missing Value Handling (Non-Zero-Variance Columns) ---\n",
      "  - High missing (>50%, fill with 0): 28 columns\n",
      "  - Low missing (≤50%, fill with mean): 394 columns\n",
      "\n",
      "Final preprocessed data shape: (11491, 474)\n",
      "Samples: 11491, Features: 474\n",
      "\n",
      "================================================================================\n",
      "PROBLEM 3: Model Development\n",
      "================================================================================\n",
      "\n",
      "Train set: 9192 samples\n",
      "Validation set: 2299 samples\n",
      "\n",
      "--- Training: Logistic Regression (L2) ---\n",
      "Optimal threshold: 0.910\n",
      "Validation - Precision: 0.9441, Recall: 0.9122\n",
      "Validation - F1: 0.9278, AUC: 0.9931\n",
      "Validation - Profit: 1,900,100 won\n",
      "\n",
      "--- Training: KNN (k=5) ---\n",
      "Optimal threshold: 0.410\n",
      "Validation - Precision: 0.9865, Recall: 0.9865\n",
      "Validation - F1: 0.9865, AUC: 0.9999\n",
      "Validation - Profit: 1,917,100 won\n",
      "\n",
      "--- Training: Random Forest ---\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
